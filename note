補reference
Decision Trees

It commonly use in real word, for example, to clarify the mail which is spam mail or important mail.
It has high interpretability which means easy to explain the result. It can creat the model of the dataset which contain labels and categories.
The major decision tree disadvantages are its complexity, especially large ones with many branches, are complex and time-consuming affairs.
Because we want to know the feature is meanful or not. Decision tree is easy to explain the result and can output of feature importance.
Stochastic Gradient Descent Classifier (SGDC)

Gradient Boosting can be applied in ranking algorithms, like ranking of searches by search engines.
The method is very good for large datasets, reduces bias and variance, combines multiple weak predictors to a build strong predictor.
Relatively high training time, over-fitting if the data sample is too small.
The data we have is sufficiently large and clean so gradient boosting is suitable in this case.
Logistic Regression

Logistic Regression is very widely used in the case of binary classification problems.
It can easily gives good results in case of less features with less training and prediction time.
It hard to handle the complex problem which features have correlation to each other.
This roblem is of binary classification with clean data, all favourable conditions for logistic regression.

{Decision tree:
https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1
https://scikit-learn.org/stable/modules/tree.html

Stochastic Gradient Descent Classifier
https://scikit-learn.org/stable/modules/sgd.html
https://en.wikipedia.org/wiki/Stochastic_gradient_descent

Logistic Regression
https://en.wikipedia.org/wiki/Logistic_regression
https://www.statisticssolutions.com/regression-analysis-logistic-regression/}


Gradient Boosting = Gradient Descent + Boosting

Gradient boosting is a machine learning method by ensemble weak prediction models and try to improve the error every time to produces a prediction model. In this method, the residue from prediction to target will be calculate and try have new function. And than the model which combine original and new function will be applied.

The Gradient Boosting is like mountain climb, we would not try go straight to the climax. We will try to find some easy way to close to the climax which means we will take longer path but every step is easier to move. The climb path will shows a gradient to the line cross from bottom of the moutain and the climax.
{


Decision trees are used as the weak learner in gradient boosting.
Specifically regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions.
Trees are constructed in a greedy manner, choosing the best split points based on purity scores like Gini or to minimize the loss.
Initially, such as in the case of AdaBoost, very short decision trees were used that only had a single split, called a decision stump. Larger trees can be used generally with 4-to-8 levels.
It is common to constrain the weak learners in specific ways, such as a maximum number of layers, nodes, splits or leaf nodes.
This is to ensure that the learners remain weak, but can still be constructed in a greedy manner

https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/}


{ code

from sklearn import grid_search
from sklearn.metrics import make_scorer, r2_score, fbeta_score
# TODO: Initialize the classifier
clf = GradientBoostingClassifier(random_state=10)

# TODO: Create the parameters list you wish to tune
parameters = {'loss': ['deviance', 'exponential'],
              'n_estimators': [100, 300, 500],
              'learning_rate': [0.1, 1, 1.3],
              'max_depth': [1,3,5,7]
             }

# TODO: Make an fbeta_score scoring object
scorer = make_scorer(fbeta_score, beta=0.5)

# TODO: Perform grid search on the classifier using 'scorer' as the scoring method
grid_obj = grid_search.GridSearchCV(clf, parameters, scoring=scorer, n_jobs=10)

# TODO: Fit the grid search object to the training data and find the optimal parameters
grid_fit = grid_obj.fit(X_train, y_train)

# Get the estimator
best_clf = grid_fit.best_estimator_

# Make predictions using the unoptimized and model
predictions = (clf.fit(X_train, y_train)).predict(X_test)
best_predictions = best_clf.predict(X_test)

# Report the before-and-afterscores
print "Unoptimized model\n------"
print "Accuracy score on testing data: {:.4f}".format(accuracy_score(y_test, predictions))
print "F-score on testing data: {:.4f}".format(fbeta_score(y_test, predictions, beta = 0.5))
print "\nOptimized Model\n------"
print "Final accuracy score on the testing data: {:.4f}".format(accuracy_score(y_test, best_predictions))
print "Final F-score on the testing data: {:.4f}".format(fbeta_score(y_test, best_predictions, beta = 0.5))
print "\nOptimized Classifier\n------"
print best_clf
}

{
